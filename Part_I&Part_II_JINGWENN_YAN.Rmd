---
title: "INFSCI 2595 Final Project: Read data"
subtitle: "Example: save and reload a model object"
author: "Jingwen Yan"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load packages

```{r, load_packages,warning=FALSE}
library(tidyverse)
```

## Overview

This RMarkdown shows how to download the final project data. It shows how to select the variables for the regression and classification portions of the final project. It also demonstrates how to fit a simple model (with `lm()`), save that model, and load it back into the workspace. You may find these actions helpful as you work through the project.  

## Final project data

The code chunk below reads in the data for the final project.  

```{r, read_glimpse_data,warning=FALSE}
data_url <- 'https://raw.githubusercontent.com/jyurko/INFSCI_2595_Fall_2020/master/HW/final_project/infsci_2595_final_project_data.csv'

df <- readr::read_csv(data_url, col_names = TRUE)
```

Get a glimpse of the data.  

```{r, check_glimpse}
df %>% glimpse()
```

Separate the variables associated with Step 1.  

```{r, make_step_1_data}
step_1_df <- df %>% select(xA, xB, x01:x06, response_1)
```

Separate the variables associated with the Option B classification formulation. Notice that the `outcome_2` variable is converted to a factor with a specific ordering of the levels. Use this ordering when modeling in `caret` to make sure everyone predicts the `Fail` class as the "positive" class in the confusion matrix.  

```{r, make_step_2_option_b_data}
step_2_b_df <- df %>% select(xA, xB, response_1, x07:x11, outcome_2) %>% 
  mutate(outcome_2 = factor(outcome_2, levels = c("Fail", "Pass")))
```

Separate the variables associated with the Option A classification formulation. The `outcome_2` variable is again converted to a factor with a specific ordering of the levels.  

```{r, make_step_2_option_a_data}
step_2_a_df <- df %>% select(xA, xB, x01:x11, outcome_2) %>% 
  mutate(outcome_2 = factor(outcome_2, levels = c("Fail", "Pass")))
```

## Simple model

Let's fit a simple linear model for `response_1`. We will use a linear relationship and only a single input, `x01` for demonstration purposes. With `lm()` the linear model is easy to fit using the formula interface, as shown below.  

```{r, a_simple_model}
mod01 <- lm(response_1 ~ x01, step_1_df)
```

Summarize the model results with the `summary()` function.  

```{r, show_model_summary}
mod01 %>% summary()
```

Let's go ahead and save `mod01`. There are multiple approaches for saving objects including `.Rda` and `.rds`. I prefer to use the `.rds` object because it's more streamlined and makes it easier to save and reload a single object, which in our case is a model object. We can use the base `R` `saveRDS()` function or the `tidyverse` equivalent `write_rds()` function from the `readr` package. I prefer to use the `tidyverse` version.  

The code chunk below pipes the `mod01` object into `readr::write_rds()`. It saves the object to a file in the local working directory for simplicity. Notice that the `.rds` extension is included after the desired file name.  


```{r, save_a_model}
mod01 %>% readr::write_rds("my_simple_example_model.rds")
```


If you ran the above code chunk, check your working directory with the Files tab. You should see the `my_simple_example_model.rds` in your current working directory.  

Let's now load in that model, but assign it to a different variable name. We can read in an `.rds` file with the 

```{r, load_back_the_model}
re_load_mod01 <- readr::read_rds("my_simple_example_model.rds")
```

We can now work with the `re_load_mod01` object just like the original model we fit, `mod01`. So we can use `summary()` and another other function on the model object, like `predict()`. To confirm let's print out the summary below.  

```{r, check_model_summary}
re_load_mod01 %>% summary()
```

And to confirm let's check that the models are in fact the same.  

```{r, check_models_same}
all.equal(mod01, re_load_mod01)
```

# Part i: Exploration

## Visualize the distribution of the variables in the data set.

```{r}
df %>% 
  tidyr::gather(key = "key", value = "value", -outcome_2, -xA, -xB) %>% 
  mutate(input_number = key) %>% 
  ggplot(mapping = aes(x = value)) +
  geom_histogram(bins = 25) +
  facet_wrap(~input_number, scales = "free") +
  theme_bw() +
  theme(axis.text.y = element_blank())
```

```{r}
df %>% 
  ggplot(mapping = aes(x = outcome_2))+ 
  geom_bar()
```

```{r}
df %>% 
  ggplot(mapping = aes(x =xA))+ 
  geom_bar()
```
```{r}
df %>% 
  ggplot(mapping = aes(x = xB))+ 
  geom_bar()
```

## Consider breaking up the continuous variables based on the discrete xA and xB

variables.

```{r}
df %>% 
  tidyr::gather(key = "key", value = "value", -outcome_2, -xA, -xB) %>% 
  mutate(input_number = key) %>% 
  ggplot(mapping = aes(x = value)) +
  geom_freqpoly(bins = 25,
                mapping = aes(color = xA,
                              y = stat(density)),
                size = 1.) +
  facet_wrap(~input_number, scales = "free") +
  scale_color_brewer(palette = "Set1") +
  theme_bw() +
  theme(axis.text.y = element_blank())
```
```{r}
df %>% 
  tidyr::gather(key = "key", value = "value", -outcome_2, -xA, -xB) %>% 
  mutate(input_number = key) %>% 
  ggplot(mapping = aes(x = value)) +
  geom_freqpoly(bins = 25,
                mapping = aes(color = xB,
                              y = stat(density)),
                size = 1.) +
  facet_wrap(~input_number, scales = "free") +
  scale_color_brewer(palette = "Set1") +
  theme_bw() +
  theme(axis.text.y = element_blank())
```
They are slightly different but roughly similar.

## Visualize the relationships between the inputs

```{r}
mod_df <- fastDummies::dummy_cols(df)
```

```{r}
mod_df %>%
  dplyr::select(x01:x11,xA_A1:xB_B4)%>%
  cor()%>%
  corrplot::corrplot(method = "square")
```
No. They are not correlated.

## Visualize the relationships between response_1 and the Step 1 inputs

```{r}
step_1 <- fastDummies::dummy_cols(step_1_df)
```

```{r}
step_1 %>%
  dplyr::select(x01:response_1,xA_A1:xB_B4)%>%
  cor()%>%
  corrplot::corrplot(method = "square")
```

## visualize the behavior of outcome_2 with respect to the Step 2 inputs and response_1

```{r}
step_2 <- step_2_b_df %>%
  mutate(outcome = ifelse(step_2_b_df$outcome_2 == "Fail",1,0))
```

```{r}
step_2 <- fastDummies::dummy_cols(step_2)
```

```{r}
step_2 %>%
  dplyr::select(response_1:x11,outcome:xB_B4)%>%
  cor()%>%
  corrplot::corrplot(method = "square")
```
Outcome has slightly relationship to response_1. And it nearly has no relationship with inputs.

# Part ii: Regression models â€“ iiA)

## discrete inputs

```{r}
mod_d <- lm(response_1 ~ xA + xB, df)
mod_d %>% summary()
```
## continuous inputs

```{r}
mod_c <- lm(response_1 ~ x01 + x02 + x03 + x04 + x05 + x06 + x07 + x08 + x09 + x10 + x11, df)
mod_c %>% summary()
```
## All step 1 inputs

```{r}
mod_step1 <- lm(response_1 ~ x01 + x02 + x03 + x04 + x05 + x06 + xA + xB, step_1_df)
mod_step1 %>% summary()
```
## All inputs.

```{r}
mod_my <- lm(response_1 ~ x01 + x02 + x03 + x04 + x05 + x06 + x07 + x08 + x09 + x10 + x11 + xA + xB, df)
mod_my %>% summary()
```
The forth one is best because it has highest Adjusted R-squared. 

```{r}
library(jtools)
```

```{r}
plot_coefs(mod_step1, mod_my, scale = TRUE, omit.coefs = "NULL")
```
We use plot_coefs to compare their coefficients. I find coefficients of x01:x11 are very similar. The intercepts are very similar also. 

# Part ii: Regression models â€“ iiB)
```{r}
library(rstanarm)
```

### stan_lm for mod_c

```{r}
stan_c <- stan_lm(response_1 ~ x01 + x02 + x03 + x04 + x05 + x06 + x07 + x08 + x09 + x10 + x11, data = df,
                 prior = R2(location = 0.5),
                 seed = 432123)
```

```{r}
stan_c %>% summary()
```

### stan_lm for mod_my

```{r}
stan_my <- stan_lm(response_1 ~ x01 + x02 + x03 + x04 + x05 + x06 + x07 + x08 + x09 + x10 + x11 + xA + xB, data = df,
                 prior = R2(location = 0.5),
                 seed = 432123)
```

```{r}
stan_my %>% summary()
```

I select R2. According to R2, the second model is best. 
R2 for stan_c: 0.2
R2 for stan_my (mean): 0.5

### Visualize the posterior distributions on the coefficients

```{r}
plot(stan_my)+ 
  geom_vline(xintercept = 0, color = "grey", linetype = "dashed", size = 1.) + 
  theme_bw()
```
```{r}
as.data.frame(stan_my) %>% tibble::as_tibble() %>% 
  select(names(stan_my$coefficients)) %>% 
  tidyr::gather(key = "key", value = "value") %>% 
  ggplot(mapping = aes(x = value)) +
  geom_histogram(bins = 55) +
  facet_wrap(~key, scales = "free") +
  theme_bw() +
  theme(axis.text.y = element_blank())
```
### the uncertainty in the noise (residual error), ðœŽ

```{r}
as.data.frame(stan_my) %>% tibble::as_tibble() %>% 
  select(sigma) %>% 
  pull() %>% 
  quantile(c(0.05, 0.5, 0.95))
```

```{r}
summary(mod_my)$sigma
```
The lm() MLE on $\sigma$ is similar with $\sigma$ with 50% uncertainty.

# Part ii: Regression models â€“ iiC)

```{r, warning=FALSE}
library(tidymodels)
```

```{r, warning=FALSE}
library(Metrics)
```

## Split data. train: 80%, test: 20%

```{r}
set.seed(123)
data_split <- initial_split(step_1_df, prop = 0.8)

train_data <- training(data_split)
test_data  <- testing(data_split)
```

## Resample: 10 folds cross-validation

```{r}
set.seed(345)
folds <- vfold_cv(train_data, v = 10)
folds
```

## Linear additive model

### fit

```{r}
set.seed(57974)
lm_fit <-
  linear_reg() %>%
  set_mode("regression") %>% 
  set_engine("lm")%>%
  fit(response_1 ~ ., data = train_data)
lm_fit
```

### preprocess with recipe

```{r}
set.seed(234)
lm_rec <- recipe(response_1 ~ ., data = train_data)%>%
  step_dummy(xA,xB)%>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  step_ns(x01, deg_free = tune("x01")) %>%
  step_ns(x02, deg_free = tune("x02"))
```

```{r}
lm_spec <- linear_reg() %>% 
  set_engine("lm")
```

```{r}
lm_wf <- 
  workflow() %>%
  add_model(lm_spec) %>%
  add_recipe(lm_rec)
```

```{r}
lm_grid <- expand.grid(x01 = 1:5, x02 = 1:5)
```

### resample and tune

```{r}
lm_tune <- tune_grid(
  lm_wf,
  resamples = folds,
  grid = lm_grid,
  control = control_grid(save_pred = TRUE)
)
```

```{r}
collect_metrics(lm_tune)
```

### best parameter

```{r}
best_lm <- lm_tune  %>%
  select_best("rmse")
best_lm
```

### Last fit model with best parameter

```{r}
lm_wf_final <- 
  lm_wf %>%
  finalize_workflow(best_lm) %>%
  fit(data = train_data)
```

```{r}
lm_last_fit <- last_fit(
  lm_wf_final,
  data_split
)

collect_metrics(lm_last_fit)
```

## Regularized regression with Elastic net

```{r, warning=FALSE}
library(glmnet)
```

### fit

```{r}
set.seed(57974)
glmnet_fit <-
  linear_reg() %>%
  set_mode("regression") %>% 
  set_engine("glmnet")%>%
  fit(response_1 ~ ., data = train_data)
glmnet_fit
```

### preprocess with recipe

```{r}
set.seed(234)
glmnet_rec <- recipe(response_1 ~ ., data = train_data)%>%
  step_dummy(xA,xB)%>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) 
```

```{r}
glmnet_spec <- linear_reg(penalty = tune(),mixture = tune()) %>% 
  set_engine("glmnet")
```

```{r}
glmnet_grid <- glmnet_spec %>%
  parameters() %>%
  grid_max_entropy(size = 10)
```

```{r}
glmnet_wf <- 
  workflow() %>%
  add_model(glmnet_spec) %>%
  add_recipe(glmnet_rec)
```

### resample and tune

```{r}
glmnet_tune <- tune_grid(
  glmnet_wf,
  resamples = folds,
  grid = glmnet_grid
)
```

```{r}
collect_metrics(glmnet_tune) 
```

```{r}
best_glmnet <- glmnet_tune  %>%
  select_best("rmse")
best_glmnet
```

### Last fit model with best parameter

```{r}
glmnet_wf_final <- 
  glmnet_wf %>%
  finalize_workflow(best_glmnet) %>%
  fit(data = train_data)
```

```{r}
glmnet_last_fit <- last_fit(
  glmnet_wf_final,
  data_split
)

collect_metrics(glmnet_last_fit)
```
### pair interaction

```{r}
set.seed(224)
glmnet_rec_2 <- recipe(response_1 ~ ., data = train_data)%>%
  step_dummy(xA,xB)%>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  step_interact(terms = response_1 ~ (.)^2)
```

```{r}
glmnet_spec_2 <- linear_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet")
```

```{r}
glmnet_grid_2 <- glmnet_spec_2 %>%
  parameters() %>%
  grid_max_entropy(size = 10)
```

```{r}
glmnet_wf_2 <- 
  workflow() %>%
  add_model(glmnet_spec_2) %>%
  add_recipe(glmnet_rec_2)
```

```{r}
glmnet_tune_2 <- tune_grid(
  glmnet_wf_2,
  resamples = folds,
  grid = glmnet_grid_2
)
```

```{r}
collect_metrics(glmnet_tune_2)
```

```{r}
best_glmnet_2 <- glmnet_tune_2  %>%
  select_best("rmse")
best_glmnet_2
```

### Last fit model with best parameter

```{r}
glmnet_wf_final_2 <- 
  glmnet_wf_2 %>%
  finalize_workflow(best_glmnet_2) %>%
  fit(data = train_data)
```

```{r}
glmnet_last_fit_2 <- last_fit(
  glmnet_wf_final_2,
  data_split
)

collect_metrics(glmnet_last_fit_2)
```

### triple interaction

```{r}
set.seed(334)
glmnet_rec_3 <- recipe(response_1 ~ ., data = train_data)%>%
  step_dummy(xA,xB)%>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  step_interact(terms = response_1 ~ (.)^3)
```

```{r}
glmnet_spec_3 <- linear_reg(penalty = tune(), mixture = tune()) %>% 
  set_engine("glmnet")
```

```{r}
glmnet_grid_3 <- glmnet_spec_3 %>%
  parameters() %>%
  grid_max_entropy(size = 10)
```

```{r}
glmnet_wf_3 <- 
  workflow() %>%
  add_model(glmnet_spec_3) %>%
  add_recipe(glmnet_rec_3)
```

```{r, warning=FALSE}
glmnet_tune_3 <- tune_grid(
  glmnet_wf_3,
  resamples = folds,
  grid = glmnet_grid_3
)
```

```{r}
collect_metrics(glmnet_tune_3)
```

```{r}
best_glmnet_3 <- glmnet_tune_3  %>%
  select_best("rmse")
best_glmnet_3
```

### Last fit model with best parameter

```{r}
glmnet_wf_final_3 <- 
  glmnet_wf_3 %>%
  finalize_workflow(best_glmnet_3) %>%
  fit(data = train_data)
```

```{r}
glmnet_last_fit_3 <- last_fit(
  glmnet_wf_final_3,
  data_split
)

collect_metrics(glmnet_last_fit_3)
```
### compare pair and triple

```{r}
metric_glmnet <- rbind (collect_metrics(glmnet_last_fit),collect_metrics(glmnet_last_fit_2),collect_metrics(glmnet_last_fit_3))
metric_glmnet
```
According to the result, the rmse and rsq are very similar. Pair interactions has slightly lower rmse and triple interaction has slightly lower rsq.

## Neural network

```{r, warning=FALSE}
library(keras)
```

```{r, warning=FALSE}
library(tensorflow)
```

### fit

```{r, warning=FALSE}
set.seed(57974)
nnet_fit <-
  mlp() %>%
  set_mode("regression") %>% 
  set_engine("keras")%>%
  fit(response_1 ~ ., data = train_data)
nnet_fit
```

### preprocess with recipe

```{r}
set.seed(234)
nnet_rec <- recipe(response_1 ~ ., data = train_data)%>%
  step_dummy(xA,xB)%>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) 
```

```{r}
nnet_spec <- mlp(hidden_units = tune(), penalty = tune()) %>% 
  set_engine("keras")%>% 
  set_mode("regression")
```

```{r}
nnet_grid <- nnet_spec %>%
  parameters() %>%
  grid_max_entropy(size = 10)
```

```{r}
nnet_wf <- 
  workflow() %>%
  add_model(nnet_spec) %>%
  add_recipe(nnet_rec)
```

### resample and tune

```{r,warning=FALSE}
nnet_tune <- tune_grid(
  nnet_wf,
  resamples = folds,
  grid = nnet_grid
)
```

```{r}
collect_metrics(nnet_tune) 
```

### best parameter

```{r}
best_nnet <- nnet_tune  %>%
  select_best("rmse")
```

### Last fit model with best parameter

```{r}
nnet_wf_final <- 
  nnet_wf %>%
  finalize_workflow(best_nnet) %>%
  fit(data = train_data)
```

```{r}
nnet_last_fit <- last_fit(
  nnet_wf_final,
  data_split
)

collect_metrics(nnet_last_fit)
```

## Random Forest

```{r, warning=FALSE}
library(ranger)
```

### fit

```{r}
set.seed(57974)
rf_fit <-
  rand_forest() %>%
  set_mode("regression") %>% 
  set_engine("ranger")%>%
  fit(response_1 ~ ., data = train_data)
rf_fit
```

### preprocess with recipe

```{r}
set.seed(234)
rf_rec <- recipe(response_1 ~ ., data = train_data)%>%
  step_dummy(xA,xB)%>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) 
```

```{r}
rf_spec <- rand_forest(mtry = tune(), trees = tune()) %>% 
  set_engine("ranger")%>% 
  set_mode("regression")
```

```{r}
rf_grid <- rf_spec %>%
  parameters() %>%
  finalize(select(train_data, -response_1)) %>% 
  grid_max_entropy(size = 10)
```

```{r}
rf_wf <- 
  workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(rf_rec)
```

### resample and tune

```{r}
rf_tune <- tune_grid(
  rf_wf,
  resamples = folds,
  grid = rf_grid
)
```

```{r}
collect_metrics(rf_tune) 
```

### best patameter

```{r}
best_rf <- rf_tune  %>%
  select_best("rmse")
```

### Last fit model with best penalty

```{r}
rf_wf_final <- 
  rf_wf %>%
  finalize_workflow(best_rf) %>%
  fit(data = train_data)
```

```{r}
rf_last_fit <- last_fit(
  rf_wf_final,
  data_split
)

collect_metrics(rf_last_fit)
```

## Gradient boosted tree

```{r,warning=FALSE}
library(xgboost)
```

```{r}
set.seed(57974)
boost_fit <-
  boost_tree() %>%
  set_mode("regression") %>% 
  set_engine("xgboost")%>%
  fit(response_1 ~ ., data = train_data)
boost_fit
```

### preprocess with recipe

```{r}
set.seed(234)
boost_rec <- recipe(response_1 ~ ., data = train_data)%>%
  step_dummy(xA,xB)%>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) 
```

```{r}
boost_spec <- boost_tree(mtry = tune(), tree = tune(),
                         learn_rate = tune(), tree_depth = tune()) %>% 
  set_engine("xgboost")%>% 
  set_mode("regression")
```

```{r}
boost_grid <- boost_spec %>%
  parameters() %>%
  finalize(select(train_data, -response_1))%>%
  grid_max_entropy(size = 10)
```

```{r}
boost_wf <- 
  workflow() %>%
  add_model(boost_spec) %>%
  add_recipe(boost_rec)
```

### resmaple and tune

```{r}
boost_tune <- tune_grid(
  boost_wf,
  resamples = folds,
  grid = boost_grid
)
```

```{r}
collect_metrics(boost_tune) 
```

```{r}
best_boost <- boost_tune  %>%
  select_best("rmse")
```

### Last fit model with best penalty

```{r}
boost_wf_final <- 
  boost_wf %>%
  finalize_workflow(best_boost) %>%
  fit(data = train_data)
```

```{r}
boost_last_fit <- last_fit(
  boost_wf_final,
  data_split
)

collect_metrics(boost_last_fit)
```

## SVM

```{r,warning=FALSE}
library(kernlab)
```

### fit 

```{r}
set.seed(57974)
svm_fit <-
  svm_rbf() %>%
  set_mode("regression") %>% 
  set_engine("kernlab")%>%
  fit(response_1 ~ ., data = train_data)
svm_fit
```

```{r}
set.seed(234)
svm_rec <- recipe(response_1 ~ ., data = train_data)%>%
  step_dummy(xA,xB)%>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) 
```

```{r}
svm_spec <- svm_rbf(cost = tune(), rbf_sigma = tune()) %>% 
  set_engine("kernlab")%>% 
  set_mode("regression")
```

```{r}
svm_grid <- svm_spec %>%
  parameters() %>%
  grid_max_entropy(size = 10)
```

```{r}
svm_wf <- 
  workflow() %>%
  add_model(svm_spec) %>%
  add_recipe(svm_rec)
```

### resmaple and tune

```{r}
svm_tune <- tune_grid(
  svm_wf,
  resamples = folds,
  grid = svm_grid
)
```

```{r}
collect_metrics(svm_tune) 
```

```{r}
best_svm <- svm_tune  %>%
  select_best("rmse")
```

### Last fit model with best penalty

```{r}
svm_wf_final <- 
  svm_wf %>%
  finalize_workflow(best_svm) %>%
  fit(data = train_data)
```

```{r}
svm_last_fit <- last_fit(
  svm_wf_final,
  data_split
)

collect_metrics(svm_last_fit)
```
## KNN

```{r,warning=FALSE}
library(kknn)
```
### fit

```{r}
set.seed(57974)
knn_fit <-
  nearest_neighbor() %>%
  set_mode("regression") %>% 
  set_engine("kknn")%>%
  fit(response_1 ~ ., data = train_data)
knn_fit
```

```{r}
set.seed(234)
knn_rec <- recipe(response_1 ~ ., data = train_data)%>%
  step_dummy(xA,xB)%>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) 
```

```{r}
knn_spec <- nearest_neighbor(neighbors = tune(), weight_func = tune(), dist_power = tune()) %>% 
  set_engine("kknn")%>% 
  set_mode("regression")
```

```{r}
knn_grid <- knn_spec %>%
  parameters() %>%
  grid_max_entropy(size = 10)
```

```{r}
knn_wf <- 
  workflow() %>%
  add_model(knn_spec) %>%
  add_recipe(knn_rec)
```

### resample and tune

```{r}
knn_tune <- tune_grid(
  knn_wf,
  resamples = folds,
  grid = knn_grid
)
```

```{r}
collect_metrics(knn_tune) 
```

### best parameter

```{r}
best_knn <- knn_tune  %>%
  select_best("rmse")
```

### Last fit model with best tuning parameter

```{r}
knn_wf_final <- 
  knn_wf %>%
  finalize_workflow(best_knn) %>%
  fit(data = train_data)
```

```{r}
knn_last_fit <- last_fit(
  knn_wf_final,
  data_split
)

collect_metrics(knn_last_fit)
```

### Final results of seven models

```{r}
metric <- rbind(collect_metrics(lm_last_fit),
      collect_metrics(glmnet_last_fit),
      collect_metrics(nnet_last_fit),
      collect_metrics(rf_last_fit),
      collect_metrics(boost_last_fit),
      collect_metrics(svm_last_fit),
      collect_metrics(knn_last_fit)
      )
metric
```

```{r}
name_model <- data.frame("Model" = c("lm","lm","glmnet","glmnet","nnet","nnet","rf","rf","boost","boost","svm","svm","knn","knn"))
name_model
```

```{r}
result <- cbind(name_model, metric)
result 
```
According to mse and accuracy, XGboost is best model. 
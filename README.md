# Project-with-Tidymodels

### Option A

Train, evaluate, and compare binary classifiers for outcome_2 as a function of the inputs: xA,xB,x01:x11.

### Option B

Train, evaluate, and compare regression models for response_1 as a function of the Step 1 inputs: xA,xB,x01:x06.
Train, evaluate, and compare binary classifiers for outcome_2as a function of the inputs: xA,xB,response_1,x07:x11.

## Part II: Exploration

â€¢Visualize the distribution of the variables in the data set.

â€¢Consider breaking up the continuous variables based on the discrete xA and xB variables.

â€¢Visualize the relationships between the inputs.

â€¢Visualize the relationships between response_1and the Step 1 inputs.

## Part II: Regression models â€“ A

â€¢response_1 as a function of the step 1 inputs using linear modeling techniques.

â€¢Use lm()to fit linear models with the entire data set. 

1. Just the discrete inputs â€“additive terms.

2. Just the continuous inputs â€“additive terms.

3. All step 1 inputs â€“additive terms.

4. All.

â€¢performance metric

â€¢Visualize the coefficient summaries for best two models. 

## Part II: Regression models â€“ B

â€¢Bayesian linear models.

â€¢Laplace Approximation approach.

â€¢rstanarmâ€™s stan_lm() function to fit full Bayesian linear models with syntax similar to Râ€™s lm()function.

â€¢performance metric

â€¢Visualize the posterior distributions on the coefficients for best model. 

â€¢the uncertainty in the noise (residual error), ğœ, lm() maximum likelihood estimate (MLE) on ğœ relate to the posterior uncertainty on ğœ

## Part II: Regression models â€“ C

â€¢Train, evaluate, tune, and compare more complex methods via resampling using tidymodels. 

1. Linear additive model (method=â€˜lmâ€™)

2. Regularized regression with Elastic net (method=â€˜glmnetâ€™).

3. Neural network

4. Random forest

5. Gradient boosted tree

6. SVM

7. KNN

## Part III and Part IV: Binary classification Option B and A

â€¢Train, evaluate, tune, and compare binary classifiers via resampling using tidymodels. 

1. Linear additive model (method=â€˜lmâ€™)

2. Regularized regression with Elastic net (method=â€˜glmnetâ€™).

3. Neural network

4. Random forest

5. Gradient boosted tree

6. SVM

7. KNN

## Part V: Interpretation and â€œoptimizationâ€

â€¢Identify the most important variables associated with best performing models.

â€¢Visualize the probability of failure as a function of most important variables.

